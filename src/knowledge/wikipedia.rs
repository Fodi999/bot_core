use reqwest::Client;
use serde::Deserialize;

#[derive(Debug, Deserialize)]
struct WikipediaSummary {
    title: String,
    extract: String,
    #[serde(default)]
    pageid: Option<u64>,
    #[serde(default)] 
    type_field: Option<String>,
}

/// –ü–æ–ª—É—á–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ Wikipedia
pub async fn fetch_wikipedia_summary(query: &str) -> Result<String, String> {
    // –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤
    let clean_query = clean_wikipedia_query(query);
    
    // –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ REST API
    match fetch_from_rest_api(&clean_query).await {
        Ok(result) => Ok(result),
        Err(e) => {
            println!("‚ö†Ô∏è REST API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {}, –ø—Ä–æ–±—É–µ–º Action API", e);
            fetch_from_action_api(&clean_query).await
        }
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ REST API Wikipedia
async fn fetch_from_rest_api(query: &str) -> Result<String, String> {
    let url = format!(
        "https://en.wikipedia.org/api/rest_v1/page/summary/{}",
        urlencoding::encode(query)
    );
    
    println!("üîç Wikipedia REST URL: {}", url);
    
    let client = Client::builder()
        .timeout(std::time::Duration::from_secs(10))
        .user_agent("Auraya-Bot/1.0 (https://github.com/auraya-bot)")
        .build()
        .map_err(|e| format!("HTTP client error: {}", e))?;

    let response = client
        .get(&url)
        .send()
        .await
        .map_err(|e| format!("Wikipedia request error: {}", e))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_default();
        return Err(format!("Wikipedia REST API status {}: {}", status, error_text));
    }

    let summary: WikipediaSummary = response
        .json()
        .await
        .map_err(|e| format!("JSON parsing error: {}", e))?;

    if summary.extract.is_empty() || summary.extract.contains("may refer to:") {
        return Err("Disambiguation page or empty summary".to_string());
    }

    Ok(format!("üìñ **{}**\n\n{}", summary.title, summary.extract))
}

/// –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ Action API Wikipedia (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)
async fn fetch_from_action_api(query: &str) -> Result<String, String> {
    let url = format!(
        "https://en.wikipedia.org/w/api.php?action=query&format=json&titles={}&prop=extracts&exintro&explaintext&exsectionformat=plain",
        urlencoding::encode(query)
    );
    
    println!("üîç Wikipedia Action URL: {}", url);
    
    let client = Client::builder()
        .timeout(std::time::Duration::from_secs(10))
        .user_agent("Auraya-Bot/1.0 (https://github.com/auraya-bot)")
        .build()
        .map_err(|e| format!("HTTP client error: {}", e))?;

    let response = client
        .get(&url)
        .send()
        .await
        .map_err(|e| format!("Wikipedia request error: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("Wikipedia Action API returned status: {}", response.status()));
    }

    let wiki_response: WikipediaResponse = response
        .json()
        .await
        .map_err(|e| format!("JSON parsing error: {}", e))?;

    if let Some(query) = wiki_response.query {
        for (_, page) in query.pages {
            if let Some(extract) = page.extract {
                if !extract.is_empty() && !extract.contains("may refer to:") {
                    let title = page.title.unwrap_or_else(|| "Wikipedia".to_string());
                    return Ok(format!("üìñ **{}**\n\n{}", title, extract));
                }
            }
        }
    }

    Err("No summary available".to_string())
}

#[derive(Debug, Deserialize)]
struct WikipediaResponse {
    query: Option<WikipediaQuery>,
}

#[derive(Debug, Deserialize)]
struct WikipediaQuery {
    pages: std::collections::HashMap<String, WikipediaPage>,
}

#[derive(Debug, Deserialize)]
struct WikipediaPage {
    extract: Option<String>,
    title: Option<String>,
}

/// –û—á–∏—â–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è Wikipedia
fn clean_wikipedia_query(query: &str) -> String {
    let query_lower = query.to_lowercase();
    
    // –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
    let cleaned = query_lower
        .replace("what is ", "")
        .replace("—á—Ç–æ —Ç–∞–∫–æ–µ ", "") 
        .replace("tell me about ", "")
        .replace("—Ä–∞—Å—Å–∫–∞–∂–∏ –æ ", "")
        .replace("explain ", "")
        .replace("–æ–±—ä—è—Å–Ω–∏ ", "")
        .replace("?", "")
        .replace("!", "")
        .trim()
        .to_string();
    
    // –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    let result = match cleaned.as_str() {
        "ai" | "–∏–∏" => "artificial_intelligence".to_string(),
        "ml" => "machine_learning".to_string(),
        "js" => "javascript".to_string(),
        "css" => "cascading_style_sheets".to_string(),
        "html" => "html".to_string(),
        "cpp" | "c++" => "c++".to_string(),
        _ => cleaned.replace(" ", "_")
    };
    
    println!("üîß –û—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{}' -> '{}'", query, result);
    result
}

/// –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏, –µ—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç
pub async fn search_wikipedia_articles(query: &str, limit: usize) -> Result<Vec<String>, String> {
    let url = format!(
        "https://en.wikipedia.org/w/api.php?action=opensearch&search={}&limit={}&format=json",
        urlencoding::encode(query),
        limit
    );
    
    println!("üîç Wikipedia Search URL: {}", url);
    
    let client = Client::builder()
        .timeout(std::time::Duration::from_secs(10))
        .user_agent("Auraya-Bot/1.0")
        .build()
        .map_err(|e| format!("HTTP client error: {}", e))?;

    let response = client
        .get(&url)
        .send()
        .await
        .map_err(|e| format!("Wikipedia search error: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("Wikipedia search API status: {}", response.status()));
    }

    let search_results: serde_json::Value = response
        .json()
        .await
        .map_err(|e| format!("JSON parsing error: {}", e))?;

    if let Some(titles) = search_results.get(1).and_then(|v| v.as_array()) {
        let results: Vec<String> = titles
            .iter()
            .filter_map(|v| v.as_str().map(|s| s.to_string()))
            .take(limit)
            .collect();
        
        Ok(results)
    } else {
        Err("No search results found".to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_clean_query() {
        assert_eq!(clean_wikipedia_query("What is Rust?"), "rust");
        assert_eq!(clean_wikipedia_query("—á—Ç–æ —Ç–∞–∫–æ–µ Python"), "python");
        assert_eq!(clean_wikipedia_query("artificial intelligence"), "artificial_intelligence");
        assert_eq!(clean_wikipedia_query("What is AI?"), "artificial_intelligence");
        assert_eq!(clean_wikipedia_query("C++"), "c++");
    }

    #[tokio::test]
    async fn test_wikipedia_search() {
        let results = search_wikipedia_articles("rust programming", 3).await;
        assert!(results.is_ok());
    }
}
